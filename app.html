<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Movie Audio Description App</title>
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Inter Font -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <!-- Tone.js for background sound -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tone/14.8.49/Tone.min.js"></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f0f4f8;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            margin: 0;
            padding: 20px;
            box-sizing: border-box;
        }
        .container {
            background-color: #ffffff;
            padding: 32px;
            border-radius: 16px;
            box-shadow: 0 10px 25px rgba(0, 0, 0, 0.1);
            text-align: center;
            max-width: 600px;
            width: 100%;
            display: flex;
            flex-direction: column;
            gap: 20px;
        }
        .message-box {
            min-height: 120px;
            background-color: #e2e8f0;
            border-radius: 8px;
            padding: 16px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.125rem; /* text-lg */
            color: #334155; /* slate-700 */
            text-align: center;
            line-height: 1.6;
            word-wrap: break-word;
        }
        .voice-icon {
            animation: pulse 1.5s infinite;
        }
        @keyframes pulse {
            0%, 100% {
                transform: scale(1);
                opacity: 1;
            }
            50% {
                transform: scale(1.05);
                opacity: 0.8;
            }
        }
        .play-pause-button {
            background-color: #4CAF50; /* Green */
            color: white;
            padding: 12px 24px;
            border: none;
            border-radius: 8px;
            font-size: 1.125rem;
            cursor: pointer;
            transition: background-color 0.3s ease;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        .play-pause-button:hover {
            background-color: #45a049;
        }
        .play-pause-button:disabled {
            background-color: #cbd5e1; /* slate-300 */
            cursor: not-allowed;
            box-shadow: none;
        }
        .loading-spinner {
            border: 4px solid #f3f3f3;
            border-top: 4px solid #3b82f6; /* blue-500 */
            border-radius: 50%;
            width: 24px;
            height: 24px;
            animation: spin 1s linear infinite;
            margin: 0 auto;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1 class="text-3xl font-bold text-gray-800">Movie Audio Description</h1>
        <p class="text-gray-600">Interact with the voice agent or use text input.</p>

        <div class="message-box" id="messageBox">
            <svg id="listeningIcon" class="w-8 h-8 text-blue-500 hidden voice-icon" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M7 4a3 3 0 016 0v4a3 3 0 11-6 0V4zm4 10.93A7.001 7.001 0 0017 8a1 1 0 10-2 0A5 5 0 015 8a1 1 0 00-2 0 7.001 7.001 0 006 6.93V17H6a1 1 0 100 2h8a1 1 0 100-2h-3v-2.07z" clip-rule="evenodd"></path></svg>
            <span id="messageText">Click "Start Voice Agent" or enter a movie/YouTube link below.</span>
        </div>

        <button id="startButton" class="bg-blue-600 hover:bg-blue-700 text-white font-semibold py-3 px-6 rounded-xl shadow-md transition duration-300 ease-in-out flex items-center justify-center gap-2">
            <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10"></path></svg>
            <span id="startButtonText">Start Voice Agent</span>
        </button>

        <div class="mt-4 flex flex-col sm:flex-row items-center gap-3">
            <input type="text" id="textInput" placeholder="Enter movie title or YouTube link" class="flex-grow w-full sm:w-auto p-3 border border-gray-300 rounded-lg focus:outline-none focus:ring-2 focus:ring-blue-500 text-gray-700">
            <button id="submitTextButton" class="bg-gray-700 hover:bg-gray-800 text-white font-semibold py-3 px-6 rounded-xl shadow-md transition duration-300 ease-in-out w-full sm:w-auto">
                Get Description
            </button>
        </div>


        <button id="playPauseButton" class="play-pause-button hidden" disabled>
            <svg id="playIcon" class="w-6 h-6" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM9.555 7.168A1 1 0 008 8v4a1 1 0 001.555.832l3-2a1 1 0 000-1.664l-3-2z" clip-rule="evenodd"></path></svg>
            <svg id="pauseIcon" class="w-6 h-6 hidden" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zM7 8a1 1 0 012 0v4a1 1 0 11-2 0V8zm5-1a1 1 0 00-1 1v4a1 1 0 102 0V8a1 1 0 00-1-1z" clip-rule="evenodd"></path></svg>
            <span id="playPauseButtonText">Play Description</span>
        </button>

        <div id="loadingSpinner" class="hidden loading-spinner"></div>
    </div>

    <script type="module">
        // Firebase imports for authentication
        import { initializeApp } from "https://www.gstatic.com/firebasejs/11.6.1/firebase-app.js";
        import { getAuth, signInAnonymously, signInWithCustomToken, onAuthStateChanged } from "https://www.gstatic.com/firebasejs/11.6.1/firebase-auth.js";
        import { getFirestore } from "https://www.gstatic.com/firebasejs/11.6.1/firebase-firestore.js";

        // DOM Elements
        const messageBox = document.getElementById('messageBox');
        const messageText = document.getElementById('messageText');
        const listeningIcon = document.getElementById('listeningIcon');
        const startButton = document.getElementById('startButton');
        const startButtonText = document.getElementById('startButtonText');
        const textInput = document.getElementById('textInput');
        const submitTextButton = document.getElementById('submitTextButton');
        const playPauseButton = document.getElementById('playPauseButton');
        const playIcon = document.getElementById('playIcon');
        const pauseIcon = document.getElementById('pauseIcon');
        const playPauseButtonText = document.getElementById('playPauseButtonText');
        const loadingSpinner = document.getElementById('loadingSpinner');

        // Firebase variables
        let app;
        let db;
        let auth;
        let userId = 'anonymous'; // Default to anonymous
        let isAuthReady = false;

        // Voice Agent State
        const STATE_IDLE = 'IDLE';
        const STATE_WAITING_FOR_MOVIE = 'WAITING_FOR_MOVIE';
        const STATE_WAITING_FOR_LENGTH = 'WAITING_FOR_LENGTH';
        const STATE_PROCESSING = 'PROCESSING';
        const STATE_READY_TO_PLAY = 'READY_TO_PLAY';

        let currentState = STATE_IDLE;
        let selectedInput = ''; // Can be movie title or YouTube URL
        let inputType = ''; // 'movie' or 'youtube'
        let speechRecognition;
        let speechSynthesisUtterance;
        let speaking = false; // Track if the agent is currently speaking
        let backgroundSynth; // Tone.js synth for background sound

        // --- Firebase Initialization ---
        // Global variables provided by the Canvas environment
        const appId = typeof __app_id !== 'undefined' ? __app_id : 'default-app-id';
        const firebaseConfig = typeof __firebase_config !== 'undefined' ? JSON.parse(__firebase_config) : null;
        const initialAuthToken = typeof __initial_auth_token !== 'undefined' ? initialAuthToken : null; // Fixed: Use initialAuthToken directly

        /**
         * Initializes Firebase and authenticates the user.
         */
        const initializeFirebase = async () => {
            if (firebaseConfig) {
                try {
                    app = initializeApp(firebaseConfig);
                    db = getFirestore(app);
                    auth = getAuth(app);

                    // Sign in using custom token if provided, otherwise anonymously
                    if (initialAuthToken) {
                        await signInWithCustomToken(auth, initialAuthToken);
                        console.log('Signed in with custom token.');
                    } else {
                        await signInAnonymously(auth);
                        console.log('Signed in anonymously.');
                    }

                    // Listen for auth state changes to get the user ID
                    onAuthStateChanged(auth, (user) => {
                        if (user) {
                            userId = user.uid;
                            console.log('User ID:', userId);
                        } else {
                            userId = crypto.randomUUID(); // Fallback for truly anonymous or unauthenticated states
                            console.log('No user signed in. Using a random UUID:', userId);
                        }
                        isAuthReady = true;
                        // For multi-user apps, mandatory to show user ID on UI.
                        // messageText.textContent = `User ID: ${userId}. ${messageText.textContent}`;
                    });

                } catch (error) {
                    console.error("Error initializing Firebase or authenticating:", error);
                    showMessage("Error: Could not initialize app services. Please try again later.");
                }
            } else {
                console.warn("Firebase config not found. App will run without Firebase services.");
                isAuthReady = true; // Still allow app to function without persistence
            }
        };

        // --- UI Utility Functions ---

        /**
         * Displays a message in the message box.
         * @param {string} msg - The message to display.
         * @param {boolean} showListeningIcon - Whether to show the listening icon.
         */
        function showMessage(msg, showListeningIcon = false) {
            messageText.textContent = msg;
            if (showListeningIcon) {
                listeningIcon.classList.remove('hidden');
            } else {
                listeningIcon.classList.add('hidden');
            }
        }

        /**
         * Shows or hides the loading spinner.
         * @param {boolean} show - True to show, false to hide.
         */
        function showLoading(show) {
            if (show) {
                loadingSpinner.classList.remove('hidden');
                startButton.disabled = true;
                submitTextButton.disabled = true;
                textInput.disabled = true;
                startButtonText.textContent = 'Processing...';
                submitTextButton.textContent = 'Processing...';
                playPauseButton.classList.add('hidden');
            } else {
                loadingSpinner.classList.add('hidden');
                startButton.disabled = false;
                submitTextButton.disabled = false;
                textInput.disabled = false;
                startButtonText.textContent = 'Start Voice Agent';
                submitTextButton.textContent = 'Get Description';
            }
        }

        /**
         * Updates the play/pause button state.
         * @param {string} state - 'play' or 'pause' or 'hidden'.
         */
        function updatePlayPauseButton(state) {
            if (state === 'hidden') {
                playPauseButton.classList.add('hidden');
                playPauseButton.disabled = true;
            } else {
                playPauseButton.classList.remove('hidden');
                playPauseButton.disabled = false;
                if (state === 'play') {
                    playIcon.classList.remove('hidden');
                    pauseIcon.classList.add('hidden');
                    playPauseButtonText.textContent = 'Play Description';
                } else if (state === 'pause') {
                    playIcon.classList.add('hidden');
                    pauseIcon.classList.remove('hidden');
                    playPauseButtonText.textContent = 'Pause Description';
                }
            }
        }

        /**
         * Checks if a string is a valid YouTube URL and extracts video ID.
         * @param {string} url - The string to check.
         * @returns {string|null} - The YouTube video ID or null if not a valid YouTube URL.
         */
        function getYouTubeVideoId(url) {
            const regex = /(?:https?:\/\/)?(?:www\.)?(?:m\.)?(?:youtube\.com|youtu\.be)\/(?:watch\?v=|embed\/|v\/|)([\w-]{11})(?:\S+)?/g;
            const match = regex.exec(url);
            return match ? match[1] : null;
        }

        // --- Web Speech API (Speech Recognition & Synthesis) ---

        /**
         * Initializes SpeechRecognition and SpeechSynthesis.
         */
        function initSpeechRecognition() {
            if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
                showMessage("Speech Recognition not supported in this browser. Please use Chrome or Edge.");
                startButton.disabled = true;
                return;
            }

            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            speechRecognition = new SpeechRecognition();
            speechRecognition.continuous = false; // Only get one result per recognition start
            speechRecognition.interimResults = false;
            speechRecognition.lang = 'en-US';

            speechSynthesisUtterance = new SpeechSynthesisUtterance();
            speechSynthesisUtterance.lang = 'en-US';
            speechSynthesisUtterance.volume = 1;
            speechSynthesisUtterance.rate = 1;
            speechSynthesisUtterance.pitch = 1;

            // Event Listeners for Speech Recognition
            speechRecognition.onresult = (event) => {
                const transcript = event.results[0][0].transcript.trim().toLowerCase();
                console.log('User said:', transcript);
                handleSpeechResult(transcript);
            };

            speechRecognition.onerror = (event) => {
                console.error('Speech recognition error:', event.error);
                if (event.error === 'no-speech') {
                    showMessage("Didn't hear anything. Please try again.");
                } else if (event.error === 'not-allowed') {
                    showMessage("Microphone access denied. Please allow microphone access for this site in your browser settings (e.g., click the camera/microphone icon in the address bar).");
                    startButton.disabled = true;
                } else {
                    showMessage(`Speech recognition error: ${event.error}. Please try again.`);
                }
                listeningIcon.classList.add('hidden'); // Hide icon on error
                // Only re-enable start button if not processing from text input
                if (currentState !== STATE_PROCESSING) {
                    startButton.disabled = false;
                    startButtonText.textContent = 'Start Voice Agent';
                }
            };

            speechRecognition.onend = () => {
                if (!speaking && currentState !== STATE_PROCESSING && currentState !== STATE_READY_TO_PLAY) {
                    listeningIcon.classList.add('hidden');
                }
            };

            // Event Listener for Speech Synthesis (Agent's voice)
            speechSynthesisUtterance.onstart = () => {
                speaking = true;
                listeningIcon.classList.add('hidden'); // Hide listening icon when agent is speaking
            };
            speechSynthesisUtterance.onend = () => {
                speaking = false;
                if (currentState === STATE_WAITING_FOR_MOVIE || currentState === STATE_WAITING_FOR_LENGTH) {
                    // If agent finished speaking and waiting for user input, restart listening
                    try {
                        speechRecognition.start();
                        listeningIcon.classList.remove('hidden'); // Show listening icon again
                    } catch (e) {
                        console.warn("Speech recognition already active or other error:", e);
                    }
                } else if (currentState === STATE_READY_TO_PLAY) {
                    // If it was the final description, manage buttons
                    updatePlayPauseButton('play');
                    if (backgroundSynth) {
                        backgroundSynth.stop();
                    }
                }
            };
            speechSynthesisUtterance.onerror = (event) => {
                console.error('Speech synthesis error:', event);
                speaking = false;
                if (backgroundSynth) {
                    backgroundSynth.stop();
                }
                showMessage("I couldn't generate the audio description. Please try again.");
                updatePlayPauseButton('hidden');
                showLoading(false); // Re-enable all inputs/buttons
                currentState = STATE_IDLE; // Reset state
            };
        }

        /**
         * Makes the voice agent speak a message.
         * @param {string} text - The text for the agent to speak.
         */
        function speak(text) {
            speechSynthesisUtterance.text = text;
            window.speechSynthesis.speak(speechSynthesisUtterance);
        }

        // --- Voice Agent Logic (State Machine) ---

        /**
         * Handles the user's spoken input based on the current state.
         * @param {string} transcript - The transcribed text from user's speech.
         */
        function handleSpeechResult(transcript) {
            switch (currentState) {
                case STATE_WAITING_FOR_MOVIE:
                    selectedInput = transcript;
                    inputType = getYouTubeVideoId(selectedInput) ? 'youtube' : 'movie';
                    const promptText = inputType === 'youtube' ? `You provided a YouTube link.` : `You selected "${selectedInput}".`;
                    showMessage(`${promptText} Now, what length of description would you like? Say "short", "medium", or "long".`, true);
                    speak(`${promptText} Now, what length of description would you like? Say "short", "medium", or "long".`);
                    currentState = STATE_WAITING_FOR_LENGTH;
                    break;
                case STATE_WAITING_FOR_LENGTH:
                    let length = 'medium'; // Default
                    if (transcript.includes('short')) {
                        length = 'short';
                    } else if (transcript.includes('medium')) {
                        length = 'medium';
                    } else if (transcript.includes('long')) {
                        length = 'long';
                    }
                    const processingMsg = inputType === 'youtube' ?
                        `Generating a simulated "${length}" description for the YouTube video...` :
                        `Generating a "${length}" description for "${selectedInput}"...`;
                    showMessage(processingMsg);
                    speak(`Alright, generating a ${length} description. Please wait a moment.`);
                    currentState = STATE_PROCESSING;
                    generateSceneDescription(selectedInput, length, inputType);
                    break;
                default:
                    // Should not happen if state machine is managed correctly
                    showMessage("Please click 'Start Voice Agent' or enter a movie/YouTube link to begin.");
                    break;
            }
        }

        /**
         * Simulates generating a scene description using the Gemini API.
         * @param {string} input - The movie title or YouTube URL.
         * @param {string} length - Desired length: 'short', 'medium', or 'long'.
         * @param {string} type - 'movie' or 'youtube'.
         */
        async function generateSceneDescription(input, length, type) {
            showLoading(true);
            updatePlayPauseButton('hidden');

            let prompt;
            if (type === 'youtube') {
                const videoId = getYouTubeVideoId(input);
                prompt = `Generate a ${length} audio description for a dramatic scene from a hypothetical video. This is a simulated description based on the concept of a YouTube video. Focus on visual elements, character actions, and emotional tone. Do not use conversational language like "the scene begins". Just provide the description.`;
                if (videoId) {
                    prompt = `Generate a ${length} audio description for a dramatic scene from a hypothetical YouTube video with ID ${videoId}. This is a simulated description. Focus on visual elements, character actions, and emotional tone. Do not use conversational language like "the scene begins". Just provide the description.`;
                }
            } else {
                prompt = `Generate a ${length} audio description for a dramatic scene from the movie "${input}". Focus on visual elements, character actions, and emotional tone as if describing it for someone who cannot see it. Do not use conversational language like "the scene begins". Just provide the description.`;
            }

            let chatHistory = [];
            chatHistory.push({ role: "user", parts: [{ text: prompt }] });
            const payload = { contents: chatHistory };

            // API key is handled by the Canvas environment for gemini-2.0-flash
            const apiKey = "AIzaSyD42lEWzExGrRsqLk3M4F90vKkOZLtD7LI";
            const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;

            try {
                const response = await fetch(apiUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });

                const result = await response.json();
                console.log("Gemini API Response:", result);

                if (result.candidates && result.candidates.length > 0 &&
                    result.candidates[0].content && result.candidates[0].content.parts &&
                    result.candidates[0].content.parts.length > 0) {
                    const descriptionText = result.candidates[0].content.parts[0].text;
                    const displayInput = type === 'youtube' ? 'YouTube video' : input;

                    // Store the description in Django backend
                    try {
                        const djangoResponse = await fetch('http://localhost:8000/api/descriptions/', {
                            method: 'POST',
                            headers: {
                                'Content-Type': 'application/json',
                            },
                            body: JSON.stringify({
                                input_text: input,
                                input_type: type,
                                description_length: length,
                                description_text: descriptionText,
                                user_id: userId
                            })
                        });

                        if (!djangoResponse.ok) {
                            console.error('Failed to store description in Django:', await djangoResponse.text());
                        } else {
                            console.log('Description stored successfully in Django');
                        }
                    } catch (error) {
                        console.error('Error storing description in Django:', error);
                    }

                    showMessage(`Description Ready for "${displayInput}": "${descriptionText.substring(0, 100)}..."`);
                    speakDescription(descriptionText);
                    currentState = STATE_READY_TO_PLAY;
                    updatePlayPauseButton('pause'); // Button starts as pause, meaning it's currently playing
                } else {
                    showMessage("I couldn't generate a description for that. Please try another input.");
                    speak("I couldn't generate a description. Please try another one.");
                    currentState = STATE_IDLE; // Reset to allow new input
                }
            } catch (error) {
                console.error('Error calling Gemini API:', error);
                showMessage("There was an error generating the description. Please check your network and try again.");
                speak("I'm sorry, I encountered an error. Please try again.");
                currentState = STATE_IDLE; // Reset
            } finally {
                showLoading(false);
            }
        }

        /**
         * Plays the generated description with a simple background sound.
         * @param {string} descriptionText - The text to speak.
         */
        function speakDescription(descriptionText) {
            // Initialize Tone.js synth for a subtle background drone
            if (!backgroundSynth) {
                backgroundSynth = new Tone.Synth().toDestination();
                backgroundSynth.oscillator.type = "sine";
                backgroundSynth.envelope.attack = 0.5;
                backgroundSynth.envelope.decay = 0.5;
                backgroundSynth.envelope.sustain = 0.8;
                backgroundSynth.envelope.release = 1;
            }

            // Start a low drone sound
            backgroundSynth.triggerAttackRelease("C2", "8n");

            speechSynthesisUtterance.text = descriptionText;
            window.speechSynthesis.speak(speechSynthesisUtterance);

            // This ensures the background sound stops when speech ends
            speechSynthesisUtterance.onend = () => {
                speaking = false;
                if (backgroundSynth) {
                    backgroundSynth.stop();
                }
                updatePlayPauseButton('play'); // Once description is done, button shows 'Play'
            };
             speechSynthesisUtterance.onpause = () => {
                speaking = false;
                if (backgroundSynth) {
                    backgroundSynth.stop();
                }
                updatePlayPauseButton('play');
            };
            speechSynthesisUtterance.onresume = () => {
                speaking = true;
                if (backgroundSynth) {
                    backgroundSynth.triggerAttackRelease("C2", "8n");
                }
                updatePlayPauseButton('pause');
            };
        }

        // --- Event Listeners ---

        startButton.addEventListener('click', () => {
            if (speechRecognition) {
                try {
                    speechRecognition.start();
                    showMessage("What movie would you like an audio description for?", true);
                    speak("What movie would you like an audio description for?");
                    currentState = STATE_WAITING_FOR_MOVIE;
                    startButton.disabled = true; // Disable voice button during voice interaction
                    submitTextButton.disabled = true; // Disable text input during voice interaction
                    textInput.disabled = true;
                } catch (e) {
                    console.warn("Speech recognition already active or other error:", e);
                    showMessage("Please wait a moment, or click the button again if nothing happens.");
                }
            } else {
                showMessage("Speech recognition is not initialized. Please refresh the page if this persists.");
            }
        });

        submitTextButton.addEventListener('click', () => {
            const inputText = textInput.value.trim();
            if (!inputText) {
                showMessage("Please enter a movie title or YouTube link.");
                return;
            }

            selectedInput = inputText;
            inputType = getYouTubeVideoId(selectedInput) ? 'youtube' : 'movie';
            const confirmationText = inputType === 'youtube' ?
                `You entered a YouTube link. Now, what length of description would you like? Say "short", "medium", or "long".` :
                `You entered "${selectedInput}". Now, what length of description would you like? Say "short", "medium", or "long".`;

            showMessage(confirmationText, true);
            speak(confirmationText);
            currentState = STATE_WAITING_FOR_LENGTH;
            startButton.disabled = true; // Disable voice button when using text input
            submitTextButton.disabled = true; // Disable text button while waiting for voice input for length
            textInput.disabled = true;

            // Ensure speech recognition starts for the length prompt
            if (speechRecognition) {
                try {
                    speechRecognition.start();
                } catch (e) {
                    console.warn("Speech recognition already active or other error on text input submit:", e);
                }
            }
        });


        playPauseButton.addEventListener('click', () => {
            if (window.speechSynthesis.speaking) {
                if (speaking) { // If currently speaking, pause
                    window.speechSynthesis.pause();
                    if (backgroundSynth) backgroundSynth.stop();
                    updatePlayPauseButton('play');
                } else { // If paused, resume
                    window.speechSynthesis.resume();
                    if (backgroundSynth) backgroundSynth.triggerAttackRelease("C2", "8n");
                    updatePlayPauseButton('pause');
                }
            } else if (currentState === STATE_READY_TO_PLAY && speechSynthesisUtterance && speechSynthesisUtterance.text) {
                // If not speaking but ready to play a generated description again
                speakDescription(speechSynthesisUtterance.text);
                updatePlayPauseButton('pause');
            }
        });

        // --- Initial Setup ---
        window.onload = () => {
            initializeFirebase(); // Initialize Firebase first
            initSpeechRecognition(); // Then initialize speech recognition
            showMessage("Click 'Start Voice Agent' or enter a movie/YouTube link to begin.");
        };

        // Ensure speech synthesis is stopped if user navigates away or refreshes
        window.addEventListener('beforeunload', () => {
            if (window.speechSynthesis.speaking) {
                window.speechSynthesis.cancel();
            }
            if (backgroundSynth) {
                backgroundSynth.dispose();
            }
        });

    </script>
</body>
</html>
